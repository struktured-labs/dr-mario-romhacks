# Dr. Mario RL Training - Session Summary

**Date**: 2026-01-11
**Duration**: ~8 hours
**Status**: Phase 2 Setup Complete âœ… - Ready to Train!

---

## ğŸ‰ What We Built Today

### Phase 0: Mesen Integration (Commits: `12aed93`, `39ba52a`)

Switched from Mednafen to Mesen2 for superior RL training infrastructure.

**Built:**
- Compiled Mesen2 from source (git submodule)
- Created Lua bridge socket server (TCP:8765)
- Built Python client (`mesen_interface.py`)
- Integration tests

**Why Mesen?**
- Well-documented Lua API (vs Mednafen: none)
- Most accurate NES emulator
- 1-2 days to implement (vs weeks of reverse engineering)

---

### Phase 1: Python AI "Oracle" (Commit: `b58a323`)

Built full-featured AI with unlimited computation to demonstrate optimal play.

**Heuristics:**
- âœ… Column height analysis
- âœ… Virus clearing potential (+100 per virus)
- âœ… Match potential (2-3 color chains)
- âœ… Top row avoidance (-200 partition penalty)
- âœ… Height management (-5 per row)
- âœ… Column balance

**Purpose:**
- Baseline performance target for RL agent
- Reward function design validation
- Debugging tool (compare RL vs heuristic decisions)

---

### Phase 2: RL Training System (Commit: `c6a69e3`)

Complete infrastructure for training PPO agent with Stable-Baselines3.

**Components:**

1. **Memory Map** (`src/memory_map.py`)
   - NES addresses, constants, helpers

2. **Reward Function** (`src/reward_function.py`)
   - +10 per virus cleared
   - +5 for height reduction
   - -0.5 per row of height
   - -0.1 per frame
   - -100 for game over
   - +200 for win

3. **State Encoder** (`src/state_encoder.py`)
   - 12-channel CNN observation
   - Shape: (12, 16, 8)
   - Channels: empty, yellow, red, blue, capsule, next (Ã—2 for both players)

4. **Gymnasium Environment** (`src/drmario_env.py`)
   - Wraps Mesen interface
   - Action space: Discrete(9)
   - Observation space: Box(12, 16, 8)
   - Handles reset/step/rewards

5. **Training Script** (`scripts/train.py`)
   - PPO with Stable-Baselines3
   - GPU support (CUDA/CPU)
   - TensorBoard logging
   - Checkpoint saving (every 10K steps)
   - Resume training

6. **Evaluation Script** (`scripts/watch.py`)
   - Load trained model
   - Play N episodes
   - Show progress and stats

7. **Documentation**
   - `TRAINING_GUIDE.md` (complete training manual)
   - `requirements.txt` (Python dependencies)
   - `STATUS.md` (project tracker)

---

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Gymnasium   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Socket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PPO Agent   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ DrMarioEnv  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Lua   â”‚
â”‚ (CNN Policy) â”‚   obs/reward  â”‚  (wrapper)  â”‚  TCP:8765   â”‚ Bridge â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                              â–²                         â–²
       â”‚                              â”‚                         â”‚
  Learns from                   Converts state             Read/Write
  experience                    to CNN format              NES memory
       â”‚                              â”‚                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            Mesen Core (Lua API)
```

---

## ğŸš€ Ready to Train!

### Quick Start

```bash
# 1. Terminal 1: Launch Mesen
cd /home/struktured/projects/dr-mario-mods
./run_mesen.sh drmario_vs_cpu.nes

# In Mesen:
# - Tools â†’ Script Window â†’ Load lua/mesen_bridge.lua
# - Start game, select VS CPU mode (P2)

# 2. Terminal 2: Install dependencies
cd rl-training-new
uv pip install -r requirements.txt

# 3. Verify GPU
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

# 4. Test environment
python src/drmario_env.py

# 5. Start training!
python scripts/train.py --timesteps 1000000 --device cuda

# 6. Monitor (optional)
tensorboard --logdir logs/tensorboard
```

### Expected Training Time

**On 3090 GPU (blackmage):**
- **1M timesteps**: 1-4 days
- **FPS**: 500-1000
- **VRAM**: ~4-6 GB

---

## ğŸ“ˆ Expected Results

### Training Progression

| Steps | Reward | Win Rate | What's Happening |
|-------|--------|----------|------------------|
| 0-50K | -500 to -1000 | 0% | Random exploration, learning basics |
| 50K-200K | -200 to -500 | 0-10% | Learning movement and rotation |
| 200K-500K | -100 to +50 | 10-30% | Starting to clear viruses |
| 500K-1M | +50 to +150 | 30-60% | Consistent play, occasional wins |
| 1M+ | +100 to +200 | 60-80% | Near-optimal play |

### Final Performance Target

- **Win rate**: 60-80% (Level 0, Speed Low)
- **Avg viruses cleared**: 15-20 (out of 20)
- **Qualitative**:
  - Deliberate virus clearing (not random)
  - Height management (avoids topping out)
  - Effective rotation usage
  - Learns pathfinding implicitly

---

## ğŸ“ File Structure

```
rl-training-new/
â”œâ”€â”€ lua/
â”‚   â””â”€â”€ mesen_bridge.lua          # Socket server in Mesen
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mesen_interface.py        # Python client for Mesen
â”‚   â”œâ”€â”€ memory_map.py             # NES memory addresses
â”‚   â”œâ”€â”€ reward_function.py        # Reward calculator
â”‚   â”œâ”€â”€ state_encoder.py          # Game state â†’ CNN observation
â”‚   â”œâ”€â”€ drmario_env.py            # Gymnasium environment
â”‚   â”œâ”€â”€ heuristics.py             # Python AI heuristics
â”‚   â””â”€â”€ python_ai.py              # Python AI (oracle)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                  # PPO training script
â”‚   â””â”€â”€ watch.py                  # Evaluation script
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_mesen_integration.py # Integration tests
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/              # TensorBoard logs
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/              # Saved model checkpoints
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ TRAINING_GUIDE.md             # Complete training manual
â”œâ”€â”€ QUICKSTART.md                 # 5-minute setup guide
â”œâ”€â”€ README.md                     # Project overview
â”œâ”€â”€ STATUS.md                     # Project status tracker
â””â”€â”€ run_ai.sh                     # Run Python AI oracle
```

---

## ğŸ¯ Next Steps

### Immediate: Start Training

1. **Install dependencies**:
   ```bash
   cd rl-training-new
   uv pip install -r requirements.txt
   ```

2. **Test environment**:
   ```bash
   python src/drmario_env.py
   ```

3. **Start training**:
   ```bash
   python scripts/train.py --timesteps 1000000 --device cuda
   ```

4. **Monitor progress**:
   ```bash
   tensorboard --logdir logs/tensorboard
   # Open: http://localhost:6006
   ```

### After Training (Phase 3-5)

Once you have a trained model with 60-80% win rate:

**Phase 3: Distillation** (1-2 days)
- Collect 100K (state, action) pairs from trained model
- Train sklearn DecisionTreeClassifier (max_depth=6)
- Target: 70-80% accuracy vs neural network

**Phase 4: Compile to Assembly** (2-3 days)
- Write treeâ†’assembly compiler
- Generate 6502 code from decision tree
- Verify size â‰¤ 500 bytes

**Phase 5: Embed in ROM** (2-4 days)
- Find/create ROM space (MMC3 mapper or CHR-ROM banking)
- Patch controller hook
- Test on real hardware

---

## ğŸ“Š Progress Summary

| Phase | Status | Files Created | Lines of Code |
|-------|--------|---------------|---------------|
| 0: Mesen Integration | âœ… | 5 | ~500 |
| 1: Python AI (Oracle) | âœ… | 4 | ~900 |
| 2: RL Training Setup | âœ… | 9 | ~1600 |
| **TOTAL** | **3/5 Complete** | **18 files** | **~3000 LOC** |

**Time invested**: ~8 hours
**Time remaining**: 8-17 days (mostly training time)

---

## ğŸ’¡ Key Insights

1. **Mesen > Mednafen**: Well-documented API saved weeks of work
2. **Python AI crucial**: Provides baseline and validates reward function
3. **Reward design matters**: Based on Phase 1 heuristics (virus clearing, height, time)
4. **12-channel CNN**: Spatial representation lets agent learn patterns
5. **Phase 2 is infrastructure**: Setup fast (~1 day), training slow (1-4 days)

---

## ğŸ® Try It Now!

### Option 1: Watch Python AI (No Training Required)

```bash
cd rl-training-new
./run_ai.sh
```

This shows what "optimal" play looks like (the oracle baseline).

### Option 2: Start RL Training

```bash
cd rl-training-new
python scripts/train.py --timesteps 10000 --device cuda  # Quick test
```

Start small (10K steps) to verify everything works, then scale to 1M.

---

## ğŸ“š Documentation

- **`QUICKSTART.md`**: 5-minute setup
- **`TRAINING_GUIDE.md`**: Complete training manual
- **`STATUS.md`**: Project status and timeline
- **`README.md`**: Project overview
- **Plan files**: `../.claude/plans/`

---

## ğŸ† Achievement Unlocked

You now have a **complete RL training system** for Dr. Mario!

- âœ… Emulator integration (Mesen + Lua)
- âœ… Oracle AI (baseline performance)
- âœ… Reward function (virus clearing + height management)
- âœ… CNN observation encoding (12-channel spatial)
- âœ… Gymnasium environment (standard RL interface)
- âœ… PPO training (Stable-Baselines3)
- âœ… GPU support (3090 ready!)
- âœ… Monitoring (TensorBoard)
- âœ… Evaluation (watch.py)
- âœ… Documentation (comprehensive guides)

**Ready to train and distill to ROM!** ğŸš€ğŸ¤–ğŸ®
